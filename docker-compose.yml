version: "3.9"

services:
  ml-flask-app:
    build: .
    container_name: ml-flask-app
    ports:
      - "5000:5000"
    restart: unless-stopped

  triton-server:
    image: nvcr.io/nvidia/tritonserver:23.10-py3
    container_name: triton-server
    ports:
      - "8000:8000"  # HTTP
      - "8001:8001"  # gRPC
      - "8002:8002"  # Metrics
    volumes:
      - ./model_repository:/models
    command: >
      tritonserver
      --model-repository=/models
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    restart: unless-stopped

