{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93106fd1",
   "metadata": {},
   "source": [
    "# **Lab Report: Quantization and pruning**\n",
    "\n",
    "### **Student information**\n",
    "- Student name: Bart Peelman\n",
    "- Student code: CODE\n",
    "\n",
    "### **Assignment description**\n",
    "The assignment was about optimizing a TensorFlow model for deployment. We trained a baseline CNN, then applied post-training quantization, quantization aware training, and weight pruning. The goal was to see how these techniques affect model size and accuracy, and to be able to convert the models to TensorFlow Lite for efficient use on mobile or embedded devices.\n",
    "\n",
    "### **Proof of work done**\n",
    "#### Lab 3: Quantization and pruning\n",
    "##### 3.1 Open the notebook\n",
    "- I have opened, observed and ran the notebook in Google Colab.\n",
    "- Then I saved and uploaded the notebook (with output visible) to my Github reopistory.\n",
    "\n",
    "#### 3.2 Follow the instructions in the notebook\n",
    "\n",
    "#### ❓Questions\n",
    "\n",
    "- What is the role of model_builder(): how does it differ from building a model manually?\n",
    "  - model_builder() is a helper function that returns a Keras model with a predefined architecture.\n",
    "  - Purpose: makes the code reusable and ensures consistent architecture across multiple experiments (baseline, pruning, quantization).\n",
    "  - Difference: Instead of manually redefining the model each time, model_builder() centralizes model creation in one place.\n",
    "\n",
    "- What is the purpose of the TensorFlow Lite format? How does it differ from the TensorFlow format?\n",
    "  - Purpose: TFLite is designed for efficient deployment on mobile, embedded, and IoT devices.\n",
    "  - Differences:\n",
    "    - Smaller file size\n",
    "    - Optimized for low-latency inference\n",
    "    - Supports quantization for reduced precision\n",
    "    - Cannot be trained further (inference-only)\n",
    "\n",
    "\n",
    "- What changes in the model's layers after making it quantization aware?\n",
    "  - Quantization aware training wraps layers with fake quantization nodes.\n",
    "  - These nodes simulate lower-precision arithmetic (e.g., int8) during training.\n",
    "  - Visible changes in the summary: Layers are wrapped with QuantizeWrapper, increasing the total parameter count slightly.\n",
    "\n",
    "- What is quantization and pruning?\n",
    "  - Quantization: Reduces the precision of model weights and activations (e.g., from float32 → int8) to decrease size and improve inference speed.\n",
    "  - Pruning: Removes unnecessary weights (sets them to zero) to create sparsity in the network, which can reduce computation and model size.\n",
    "\n",
    "- Why should you use quantization aware training instead of simply quantizing a model after training?\n",
    "  - Post-training quantization can cause loss of accuracy, especially for small or sensitive models.\n",
    "  - Quantization aware training allows the model to adapt to precision loss during training, resulting in higher accuracy after quantization.\n",
    "\n",
    "- When do you see a difference in the model's size when using quantization: after conversion to TFLite of after model compression using gzip? Why is that?\n",
    "  - After TFLite conversion: Significant size reduction occurs because TFLite supports lower-precision weights.\n",
    "  - Gzip compression: Can reduce file size for all formats, but TFLite already optimizes for size, so gzip provides smaller additional gains.\n",
    "  - Reason: Quantization reduces raw data size, whereas gzip only compresses the file representation.\n",
    "\n",
    "- And when in the case of pruning: after conversion or after compression? Why is that?\n",
    "  - After compression (gzip) you see the size reduction for pruned models.\n",
    "  - Reason: Pruning adds sparsity (zeros in weights), which TFLite alone does not always compress efficiently; gzip takes advantage of repeated zeros.\n",
    "\n",
    "- What is the role of the sparsity and step parameters in the PolynomialDecay function?\n",
    "  - sparsity: defines the fraction of weights to prune.\n",
    "    - initial_sparsity: starting fraction (e.g., 0.0 or 50%)\n",
    "    - final_sparsity: target fraction after pruning\n",
    "\n",
    "  - step: determines how pruning progresses over training steps.\n",
    "    - begin_step: step to start pruning\n",
    "    - end_step: step to stop pruning\n",
    "\n",
    "  - PolynomialDecay gradually increases sparsity from initial → final over the steps.\n",
    "\n",
    "- Why do we need to remove the pruning layer before saving the model?\n",
    "  - Pruning layers (PruneLowMagnitude) are training-time wrappers.\n",
    "  - If saved with wrappers:\n",
    "    - Model cannot be deployed efficiently\n",
    "    - File size may increase\n",
    "  - strip_pruning() removes wrappers, leaving a standard Keras model with pruned weights ready for deployment.\n",
    "\n",
    "\n",
    "\n",
    "### **Evaluation criteria**\n",
    "- ✅ Show that you've executed the notebook and pushed it to the repository  \n",
    "- ✅ Show that you can convert a TensorFlow model to a TensorFlow Lite model  \n",
    "- ✅ Show that you can execute post-training quantization on a model  \n",
    "- ✅ Show that you can train a quantization aware model  \n",
    "- ✅ Show that you can perform weight pruning on a model  \n",
    "- ✅ Show that you wrote an elaborate lab report in Markdown and pushed it to the repository  \n",
    "- ✅ Provide an answer to all questions marked with ❓, use code to support your answers where applicable  \n",
    "  - ✅ Discuss the answers during the demo session  \n",
    "\n",
    "\n",
    "\n",
    "### **Issues**\n",
    "- **TensorFlow version:** The notebook needed TF 2.14 to work with `tensorflow_model_optimization`. Colab didn’t have it, so I ran it locally with TF 2.14.1 and TFMOT 0.8.0. Fixed pruning and quantization errors.  \n",
    "- **Quantization aware training:** `quantize_model()` didn’t work in Colab due to version issues. Worked fine after using TF 2.14.1 locally.  \n",
    "- **VS Code virtual environment:** My venv wasn’t detected automatically, so I manually selected it as the kernel.\n",
    "- **Pruning callbacks:** Needed `UpdatePruningStep()` callback in `model.fit()` to make pruning actually work.\n",
    "\n",
    "### **Reflection**\n",
    "The hardest part was fixing the version issues for pruning and quantization. Setting up the venv and kernel in VS Code was a bit tricky too. The easier part was actually training and converting the models once everything was set up. I learned a lot about quantization, pruning, and TFLite conversion. Next time, I’d make sure the environment is fully compatible before starting the notebook to save time.\n",
    "\n",
    "### **Resources**\n",
    "List all sources of useful information that you encountered while completing this assignment: books, manuals, HOWTO's, blog posts, etc. Note: AI is not considered a valid literary source. Do not cite, for example, https://chatgpt.com. If you use AI, let it guide you to real, reliable sources instead"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
